{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression we are trying to approximate a system using a linear combination of inputs.  Given a set of inputs and outputs, we want to produce something like:\n",
    "\n",
    "$$y(x) = a + bx$$\n",
    "\n",
    "To evaluate the model that we use, we need a function which provides an indication of how far off our estimate was from the true y values.  The most common of these is squared error:\n",
    "\n",
    "$$Error(a, b) = {E}[(y - y(x))^2]$$\n",
    "\n",
    "It is possible to solve this directly, but it is computationally expensive.  In fact it can often be faster to approximate the answer through gradient descent as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def squaredErrors(y, yhat):\n",
    "    if len(y) != len(yhat): raise \"Length of y and yhat has to be the same\"\n",
    "        \n",
    "    sqrErrors = [ math.pow(i[0] - i[1], 2) for i in zip(y, yhat) ]\n",
    "    return sum(sqrErrors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LinearModel:\n",
    "    def __init__(self,x, y, errorFunction = squaredErrors):\n",
    "        self.x = x\n",
    "        self.a = 0\n",
    "        self.y = y\n",
    "        self.errorFunction = squaredErrors\n",
    "    def generate_yhat(self,b):\n",
    "        return [self.a + b * xPoint for xPoint in self.x]\n",
    "    def generate_errors(self,b):\n",
    "        return self.errorFunction(self.y, self.generate_yhat(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest linear model, we fix a at 0 so that the model is constrained to go through the origin at (0,0).  This means there is just one variable that's free to change, namely b.  We can find the optimum b by taking the current b and looking just  to either side of it and seeing if that increases of decreases the error.  We reach an optimum when both sides of b lead to an increase in error.  Note the linear nature of the model ensures that we'll reach a single global optimum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simpleGradientDescent(model, b = 0, increment = 0.1):\n",
    "    \n",
    "    def direction_of_change(current, up, down):\n",
    "        if current < up and current < down:\n",
    "            return 0\n",
    "        elif current > up and current < down:\n",
    "            return 1\n",
    "        else: return -1\n",
    "        \n",
    "    iteration = b\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        current,incrDown,incrUp = model.generate_errors(iteration), model.generate_errors(iteration - increment), model.generate_errors(iteration + increment)\n",
    "        direction = direction_of_change(current, incrUp, incrDown)\n",
    "        if direction == 0: finished = True\n",
    "        else:\n",
    "            iteration = iteration + (increment * direction)\n",
    "    return iteration\n",
    "    \n",
    "x = [1,2,3,4,5,6]\n",
    "y = [1,2,3,4,5,6]\n",
    "model = LinearModel(x, y)\n",
    "simpleGradientDescent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this converges to the correct value relatively quickly.  This is overly simple in a number of regards, and we will now work on improving those simplicities.  The first is the speed of convergence to the optimum.  Initially, the speed is much too slow.  We are too cautious in moving such a small distance.  Later on we end up moving too quickly.  As we get closer to the true value we would like to make smaller changes to find the exact optimum.  This problem was masked in the case above because the numbers were chosen to work out well but consider the case below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0499999999999998"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [1,2,3,4,5,6]\n",
    "y = [1,2,3,4,5,6]\n",
    "model = LinearModel(x, y)\n",
    "simpleGradientDescent(model, b = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an increment of fixed size, we can end up half the distance of the incremet from the true value.  To solve this, we need to make the increment a function of the how far from the optimum we are.  A simple way to do this is to use the gradient of the error function as a proxy for how quickly we can move.  If the gradient is steep we can move quickly as we are far from the max, if the gradient is shallow we need to move more slowly because we are close to the centre (here we're using the fact that the error is normally distributed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Error(b) = \\frac{1}{N} \\sum\\limits_{n=1}^n (y^n - bx^n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{dError(b)}{db} = -2 \\frac{1}{N}\\sum\\limits_{n=1}^n (y^n - bx^n)x^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need is a fucntion to calculate the gradient of the error given y, x and b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradientError(y, x, b):\n",
    "    insideSummation = [  (i[1] - b * i[0]) * i[0] for i in zip(x,y) ]\n",
    "    summation = sum(insideSummation)\n",
    "    return (summation * (-2)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.333333333333332"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradientError(x, y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GradientAscentLinearRegression:\n",
    "    def __init__(self,x, y, errorFunction = squaredErrors, learningRate = 0.01):\n",
    "        self.x, self.y = x, y\n",
    "        self.errorFunction = squaredErrors\n",
    "        self.b = 1.5\n",
    "        self.learningRate = learningRate\n",
    "    def next_b(self):\n",
    "        change = gradientError(self.y, self.x, self.b)\n",
    "        return self.b - change * self.learningRate\n",
    "    def learn(self):\n",
    "        values = []\n",
    "        iterations = 0\n",
    "        while True:\n",
    "            next_b = self.next_b()\n",
    "            values.append(self.b)\n",
    "            if math.fabs((next_b - self.b))  < 0.00001 or iterations > 100:\n",
    "                break\n",
    "            self.b = next_b\n",
    "            iterations +=1 \n",
    "        return values\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.15166666666666662\n",
      "-0.1056611111111112\n",
      "-0.07361057407407401\n",
      "-0.051282033271604854\n",
      "-0.035726483179218116\n",
      "-0.024889449948188647\n",
      "-0.01733965013057137\n",
      "-0.012079956257631475\n",
      "-0.00841570285948312\n",
      "-0.00586293965877327\n",
      "-0.004084514628945435\n",
      "-0.0028455451914985996\n",
      "-0.0019823964834106977\n",
      "-0.001381069550109526\n",
      "-0.0009621451199095521\n",
      "-0.0006702944335370553\n",
      "-0.00046697178869758993\n",
      "-0.0003253236794591441\n",
      "-0.00022664216335654963\n",
      "-0.00015789404047183986\n",
      "-0.00010999951486190263\n",
      "-7.66329953538758e-05\n",
      "-5.338765342988161e-05\n",
      "-3.719339855612347e-05\n",
      "-2.5911400994171885e-05\n",
      "-1.8051609359304166e-05\n",
      "-1.2575954520199772e-05\n",
      "-8.761248315725112e-06\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "regression = GradientAscentLinearRegression(x, y)\n",
    "learning = regression.learn()\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the algorithm converges, it is important to choose an appropriate learning rate.  If we adjust the value for b too quickly, we are likely to pendulum back and forth past the maximum without ever settling.  This can be seen in the graphs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "slowRegression = GradientAscentLinearRegression(x, y, learningRate = 0.01)\n",
    "slowLearning = slowRegression.learn()\n",
    "tooFastRegression = GradientAscentLinearRegression(x, y, learningRate = 0.09)\n",
    "tooFastLearning = tooFastRegression.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 1.3483333333333334, 1.2426722222222222, 1.1690616481481482, 1.1177796148765433, 1.0820531316973252, 1.0571636817491366, 1.0398240316185652, 1.0277440753609337, 1.0193283725014506, 1.0134654328426773, 1.0093809182137319, 1.0065353730222333, 1.0045529765388226, 1.003171906988713, 1.0022097618688035, 1.0015394674352665, 1.0010724956465689, 1.0007471719671097, 1.0005205298037532, 1.0003626357632813, 1.0002526362484194, 1.0001760032530655, 1.0001226155996357, 1.0000854222010795, 1.0000595108000854, 1.000041459190726, 1.0000288832362059] [1.5, -13.666666666666666, 431.2222222222222, -12618.85185185185, 370183.32098765427, -10858680.415637858, 318521322.52537715, -9343292097.07773, 274069901544.61343, -8039383778611.66, 235821924172639.0, -6917443109064046.0, 2.029116645325454e+17, -5.952075492954665e+18, 1.7459421446000352e+20, -5.121430290826769e+21, 1.5022862186425189e+23, -4.4067062413513886e+24, 1.2926338307964075e+26, -3.791725903669462e+27, 1.1122395984097087e+29, -3.26256948866848e+30, 9.57020383342754e+31, -2.807259791138745e+33, 8.2346287206736515e+34, -2.4154910913976042e+36, 7.085440534766306e+37, -2.0783958901981168e+39, 6.096627944581142e+40, -1.788344197077135e+42, 5.245809644759595e+43, -1.5387708291294813e+45, 4.5137277654464787e+46, -1.3240268111976337e+48, 3.883811979513059e+49, -1.1392515139904973e+51, 3.341804441038791e+52, -9.802626360380453e+53, 2.8754370657115993e+55, -8.434615392754025e+56, 2.4741538485411807e+58, -7.257517955720798e+59, 2.1288719336781008e+61, -6.244691005455762e+62, 1.8317760282670235e+64, -5.373209682916602e+65, 1.5761415069888696e+67, -4.623348420500685e+68, 1.3561822033468678e+70, -3.9781344631508116e+71, 1.166919442524238e+73, -3.4229636980710976e+74, 1.0040693514341885e+76, -2.9452700975402864e+77, 8.63945895278484e+78, -2.5342412928168864e+80, 7.433774458929533e+81, -2.1805738412859964e+83, 6.396349934438923e+84, -1.8762626474354172e+86, 5.5037037658105565e+87, -1.6144197713044302e+89, 4.735631329159662e+90, -1.3891185232201675e+92, 4.074747668112492e+93, -1.1952593159796644e+95, 3.506093993540349e+96, -1.0284542381051691e+98, 3.0167990984418296e+99, -8.849277355429367e+100, 2.595788024259281e+102, -7.614311537827226e+103, 2.2335313844293197e+105, -6.55169206099267e+106, 1.9218296712245165e+108, -5.637367035591914e+109, 1.6536276637736283e+111, -4.85064114706931e+112, 1.4228547364736644e+114, -4.1737072269894156e+115, 1.2242874532502285e+117, -3.5912431962006702e+118, 1.0534313375521966e+120, -3.090065256819777e+121, 9.064191420004679e+122, -2.6588294832013725e+124, 7.799233150724025e+125, -2.2877750575457143e+127, 6.7108068354674284e+128, -1.9685033384037788e+130, 5.774276459317751e+131, -1.6937877613998737e+133, 4.9684441001062965e+134, -1.4574102693645137e+136, 4.2750701234692405e+137, -1.254020569550977e+139, 3.678460337349533e+140, -1.0790150322891963e+142, 3.165110761381643e+143, -9.28432490005282e+144, 2.723401970682161e+146, -7.988645780667673e+147]\n"
     ]
    }
   ],
   "source": [
    "print(slowLearning, tooFastLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x70da080>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXUAAAEACAYAAABMEua6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7060898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(np.arange(0,len(slowLearning)), slowLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7a04c50>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEGCAYAAABxfL6kAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwljsklSY/wfApZf+HuQSUgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x79a8630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " plt.scatter(np.arange(0,len(tooFastLearning)), tooFastLearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
